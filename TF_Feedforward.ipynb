{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import glob\n",
    "import tempfile\n",
    "import os\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_LOC = '../HMPDataset/'\n",
    "activity_list = [i for i in glob.glob(f'{IMAGE_LOC}/*') if i.find('_') > 0 and \"MODEL\" not in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(activity_list):\n",
    "    '''Read files in each activity in activity_list\n",
    "    Returns dict: key = activity name -> array of numpy arrays of shape (n_observations, 3) = (ax,ay,az)\n",
    "    '''\n",
    "    data = {}\n",
    "    \n",
    "    for t in activity_list: #loop over each activity type\n",
    "        activity_name = t.split('/')[-1]\n",
    "        data[activity_name] = []\n",
    "    \n",
    "        filenames = glob.glob(t + '/*')\n",
    "        \n",
    "        for f in filenames: #loop over every participants time-series\n",
    "            df = pd.read_csv(f, sep=' ', header=None)\n",
    "            \n",
    "            #ts = np.sqrt((df**2).sum(axis=1)) #magnitude of acceleration vector\n",
    "            \n",
    "            data[activity_name].append(np.array(df))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_acceleration_timeseries(data):\n",
    "    '''Input: data returned by read_data\n",
    "    Output: dictionary mapping activity name -> list of single time-series of acceleration magnitudes\n",
    "    '''\n",
    "    \n",
    "    data_ts = {}\n",
    "    \n",
    "    for k in data:\n",
    "        data_ts[k] = []\n",
    "        \n",
    "        for sample in data[k]: #(ax, ay, az)\n",
    "            data_ts[k].append(np.sqrt((sample**2).sum(axis=1)))\n",
    "    \n",
    "    return data_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(activity_list)\n",
    "data_ts = get_acceleration_timeseries(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_stats(params):\n",
    "    ts = params[0]\n",
    "    label = params[1]\n",
    "    bins = params[2]\n",
    "    #basic statistical measures\n",
    "    mean = np.mean(ts)\n",
    "    median = np.median(ts)\n",
    "    std = np.std(ts)\n",
    "    length = len(ts)\n",
    "    kurtosis = scipy.stats.kurtosis(ts)\n",
    "    \n",
    "    n,b,p = plt.hist(ts, bins=bins)\n",
    "    n = np.array(n)/float(np.sum(n)) #normalize i.e. fraction of entries in each bin\n",
    "    \n",
    "    if median == 0: \n",
    "        features = {'mean_over_median': 0, #dimensionless            \n",
    "                    'std_over_median': 0, #dimensionless            \n",
    "                    'length': length,\n",
    "                    'kurtosis': kurtosis, #already dimensionless by definition\n",
    "                   }\n",
    "        \n",
    "    else: \n",
    "        features = {'mean_over_median': mean/median, #dimensionless            \n",
    "            'std_over_median': std/median, #dimensionless            \n",
    "            'length': length,\n",
    "            'kurtosis': kurtosis, #already dimensionless by definition\n",
    "           }\n",
    "        \n",
    "    for i, val in enumerate(n):\n",
    "        features[f'binfrac_{i}'] = val\n",
    "    \n",
    "    features['label'] = label\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0,100,10)\n",
    "data_params = []\n",
    "for k in data_ts:\n",
    "    for elem in data_ts[k]:\n",
    "        data_params.append((elem,k,bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03418058156967163 minutes\n"
     ]
    }
   ],
   "source": [
    "then = time.time()\n",
    "pool = mp.Pool(processes=8)\n",
    "results = pool.map(featurize_stats,data_params)\n",
    "print((time.time()-then)/60, \"minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcliffor/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(results) \n",
    "train_df, test_df = train_test_split(results, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = np.array(train_df['label'])\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoder = label_encoder.fit_transform(one_hot)\n",
    "one_hot_encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "integer_encoder = integer_encoder.reshape(len(integer_encoder),1)\n",
    "train_label = one_hot_encoder.fit_transform(integer_encoder)\n",
    "\n",
    "one_hot = np.array(test_df['label'])\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoder = label_encoder.fit_transform(one_hot)\n",
    "one_hot_encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "integer_encoder = integer_encoder.reshape(len(integer_encoder),1)\n",
    "test_label = one_hot_encoder.fit_transform(integer_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(13,)),\n",
    "    keras.layers.Dense(14, activation=tf.nn.log_softmax)\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm = tf.train.AdamOptimizer(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adm, \n",
    "              loss=tf.losses.softmax_cross_entropy,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit( train_df.drop('label', axis=1), train_label, epochs=80000, batch_size=629, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'], label = 'train_accuracy',)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_df.drop('label', axis=1), test_label)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added code snippet from https://www.tensorflow.org/serving/tutorials/Serving_REST_simple\n",
    "# to save model for TF-serving\n",
    "\n",
    "save = False\n",
    "\n",
    "if save == True:\n",
    "    MODEL_DIR = tempfile.gettempdir()\n",
    "    version = 1\n",
    "    export_path = os.path.join(MODEL_DIR, str(version))\n",
    "    print('export_path = {}\\n'.format(export_path))\n",
    "    if os.path.isdir(export_path):\n",
    "        print('\\nAlready saved a model, cleaning up\\n')\n",
    "        !rm -r {export_path}\n",
    "\n",
    "    tf.saved_model.simple_save(\n",
    "        keras.backend.get_session(),\n",
    "        export_path,\n",
    "        inputs={'input_data': model.input},\n",
    "        outputs={t.name:t for t in model.outputs})\n",
    "\n",
    "    print('\\nSaved model:')\n",
    "    !ls -l {export_path}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
